{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-class Classification - Dicoding Pt.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODvFun8XqZfQ8asB3rFRmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blankon123/random-analytics/blob/main/Multi_class_Classification_Dicoding_Pt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B341d_I3Ucq5"
      },
      "source": [
        "# Multiclass Text Classification with LSTM\n",
        "Dalam rangka mengerjakan projek mandiri Intermediate Machine Learning - Indosat Ooredoo Scholarship di dicoding. Data yang digunakan yakni data teks berita BBC-News yang sudah dikategorisasi yang berasal dari [Public Dataset](http://mlg.ucd.ie/datasets/bbc.html). Data terdiri dari 2225 teks dengan 5 kelas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzSAT25MJbtu"
      },
      "source": [
        "## Loading Data\n",
        "Memuat data dengan download lalu uncompress file, setelah itu dilakukan pembacaan berdasarkan file per folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKsDI9suhps2",
        "outputId": "f5fd6547-8e54-44f5-f22b-d4357ff5c4ae"
      },
      "source": [
        "#Memastikan menggunakan GPU dalam komputasinya\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LvsxH_Pw6xQ",
        "outputId": "0a85e433-08d8-43a6-fd4f-4cfbaed640d2"
      },
      "source": [
        "#Download data teks yang dibutuhkan\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!wget 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "!unzip \"bbc-fulltext.zip\" -d \"/content/drive/\"\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print('Terdownload di /content/drive/bbc [1 Folder 1 Kelas]')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Terdownload di /content/drive/bbc [1 Folder 1 Kelas]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLTQ6FAfk7Op"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdnODr0axECF"
      },
      "source": [
        "from glob import glob\n",
        "import re\n",
        "\n",
        "raw_data = []\n",
        "teks = ''\n",
        "\n",
        "kelas = [x.split('/')[4] for x in glob(\"/content/drive/bbc/*/\")]\n",
        "\n",
        "#Loop Read ke semua file txt berdasarkan kelas\n",
        "for k in kelas:\n",
        "  daftar_file = glob(\"/content/drive/bbc/\"+k+\"/*\")\n",
        "  for teks_file in daftar_file:\n",
        "    with open(teks_file, 'r',encoding='latin-1') as file:\n",
        "      teks = file.read().replace('\\n', ' ').replace('  ', ' ').lower()\n",
        "    raw_data.append({\n",
        "        'teks' : re.sub(r'[^\\w\\s]', '', teks),\n",
        "        'kelas' : k\n",
        "    })"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUKInyIkewfn",
        "outputId": "d9928215-5d84-4a81-d61b-b989c031bef7"
      },
      "source": [
        "#Download Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgUCM_rUeHj5",
        "outputId": "7fe2727e-de89-49d0-a17a-8ea0b7d3cc2b"
      },
      "source": [
        "#Penghapusan stopwords, karena sepertinya pemodelan topik hanya membutuhkan keyword-keyword penting\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_stopwords(teks):\n",
        "  filtered_ = [word for word in teks.split() if word not in stopwords.words('english')]\n",
        "  return ' '.join(filtered_)\n",
        "\n",
        "for i in raw_data:\n",
        "  i['teks'] = clean_stopwords(i['teks'])\n",
        "\n",
        "raw_data[1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kelas': 'entertainment',\n",
              " 'teks': 'housewives lift channel 4 ratings debut us television hit desperate housewives helped lift channel 4s january audience share 12 compared last year successes celebrity big brother simpsons enabled broadcaster surpass bbc two first month since last july bbc twos share audience fell 112 96 last month comparison january 2004 celebrity big brother attracted fewer viewers 2002 series comedy drama desperate housewives managed pull five million viewers one point run date attracting quarter television audience two main television channels bbc1 itv1 seen monthly audience share decline year year comparison january fives proportion remained slender 63 digital multichannel tv continuing strongest area growth bbc reporting freeview box ownership five million including one million sales last portion 2004 share audience soared 20 january 2005 compared last year currently stands average 286'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_Ak_ZETqOuD5",
        "outputId": "b0adb0b8-4ab8-4aa8-c5de-3caeb56ac318"
      },
      "source": [
        "#Konversi ke Pandas Dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(raw_data)\n",
        "df.head(8)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>teks</th>\n",
              "      <th>kelas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>godzilla gets hollywood fame star movie monste...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>housewives lift channel 4 ratings debut us tel...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>boogeyman takes box office lead lowbudget horr...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spike lee backs student directors filmmaker sp...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dvd review robot one man recognises robots thr...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>da vinci code lousy history plot international...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>uganda bans vagina monologues ugandas authorit...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>prince crowned top music earner prince earned ...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                teks          kelas\n",
              "0  godzilla gets hollywood fame star movie monste...  entertainment\n",
              "1  housewives lift channel 4 ratings debut us tel...  entertainment\n",
              "2  boogeyman takes box office lead lowbudget horr...  entertainment\n",
              "3  spike lee backs student directors filmmaker sp...  entertainment\n",
              "4  dvd review robot one man recognises robots thr...  entertainment\n",
              "5  da vinci code lousy history plot international...  entertainment\n",
              "6  uganda bans vagina monologues ugandas authorit...  entertainment\n",
              "7  prince crowned top music earner prince earned ...  entertainment"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK2Bso7QO7mk",
        "outputId": "8bf3d314-3847-47af-edeb-4af4d5c9249b"
      },
      "source": [
        "# Memastikan jumlah kelas\n",
        "df.kelas.unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['entertainment', 'politics', 'sport', 'business', 'tech'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gpgLHCALTnC6",
        "outputId": "3cdd2c9c-4e1d-4245-b010-d8332ec800c2"
      },
      "source": [
        "# One hot encoding\n",
        "kategori = pd.get_dummies(df.kelas)\n",
        "df = pd.concat([df, kategori], axis=1) \n",
        "df = df.drop(columns='kelas')\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>teks</th>\n",
              "      <th>business</th>\n",
              "      <th>entertainment</th>\n",
              "      <th>politics</th>\n",
              "      <th>sport</th>\n",
              "      <th>tech</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>godzilla gets hollywood fame star movie monste...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>housewives lift channel 4 ratings debut us tel...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>boogeyman takes box office lead lowbudget horr...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spike lee backs student directors filmmaker sp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dvd review robot one man recognises robots thr...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                teks  business  ...  sport  tech\n",
              "0  godzilla gets hollywood fame star movie monste...         0  ...      0     0\n",
              "1  housewives lift channel 4 ratings debut us tel...         0  ...      0     0\n",
              "2  boogeyman takes box office lead lowbudget horr...         0  ...      0     0\n",
              "3  spike lee backs student directors filmmaker sp...         0  ...      0     0\n",
              "4  dvd review robot one man recognises robots thr...         0  ...      0     0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOBYBMgvlAfB"
      },
      "source": [
        "## Train-Test Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Y-hTHJT_bw"
      },
      "source": [
        "#Pemisahan label dan atribut\n",
        "berita = df.teks.values \n",
        "label = df[kategori.columns.values].values "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmfTrrzxUXW6"
      },
      "source": [
        "#Pemecahan train dan test-set\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "berita_latih, berita_test, label_latih, label_test = train_test_split(berita, label, test_size=0.2,shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OgI-2SglEco"
      },
      "source": [
        "## Text to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ICMCawiVtVu"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='-') \n",
        "tokenizer.fit_on_texts(berita_latih)  \n",
        "tokenizer.fit_on_texts(berita_test) \n",
        "\n",
        "sekuens_latih = tokenizer.texts_to_sequences(berita_latih) \n",
        "sekuens_test = tokenizer.texts_to_sequences(berita_test) \n",
        "\n",
        "padded_latih = pad_sequences(sekuens_latih)\n",
        "padded_test = pad_sequences(sekuens_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYdU81SlJ_P"
      },
      "source": [
        "## Neural Net Design"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSa_59fNV8rO"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "model = tf.keras.Sequential([ \n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=16), \n",
        "    tf.keras.layers.LSTM(64), \n",
        "    tf.keras.layers.Dense(128, activation='relu'), \n",
        "    tf.keras.layers.Dense(64, activation='relu'), \n",
        "    tf.keras.layers.Dense(5, activation='softmax') \n",
        "]) \n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK6-9RzClNXh"
      },
      "source": [
        "## Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBw96kwJWGtv",
        "outputId": "9024be99-ea74-4ea3-bc39-c1d86862abb1"
      },
      "source": [
        "num_epochs = 30 \n",
        "\n",
        "history = model.fit(padded_latih, label_latih,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_data=(padded_test, label_test),\n",
        "                    verbose=2\n",
        "                    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "56/56 - 12s - loss: 1.6026 - accuracy: 0.2646 - val_loss: 1.5796 - val_accuracy: 0.4135\n",
            "Epoch 2/30\n",
            "56/56 - 4s - loss: 1.3220 - accuracy: 0.4236 - val_loss: 1.0538 - val_accuracy: 0.5258\n",
            "Epoch 3/30\n",
            "56/56 - 4s - loss: 0.9455 - accuracy: 0.5629 - val_loss: 0.9351 - val_accuracy: 0.5685\n",
            "Epoch 4/30\n",
            "56/56 - 4s - loss: 0.6870 - accuracy: 0.6865 - val_loss: 0.8757 - val_accuracy: 0.6337\n",
            "Epoch 5/30\n",
            "56/56 - 4s - loss: 0.4371 - accuracy: 0.8056 - val_loss: 0.8265 - val_accuracy: 0.6966\n",
            "Epoch 6/30\n",
            "56/56 - 4s - loss: 0.3237 - accuracy: 0.8899 - val_loss: 0.9816 - val_accuracy: 0.6584\n",
            "Epoch 7/30\n",
            "56/56 - 4s - loss: 0.1980 - accuracy: 0.9438 - val_loss: 0.9865 - val_accuracy: 0.7146\n",
            "Epoch 8/30\n",
            "56/56 - 4s - loss: 0.0817 - accuracy: 0.9775 - val_loss: 0.9545 - val_accuracy: 0.7640\n",
            "Epoch 9/30\n",
            "56/56 - 4s - loss: 0.0470 - accuracy: 0.9888 - val_loss: 1.0336 - val_accuracy: 0.7685\n",
            "Epoch 10/30\n",
            "56/56 - 4s - loss: 0.0312 - accuracy: 0.9899 - val_loss: 1.1154 - val_accuracy: 0.7820\n",
            "Epoch 11/30\n",
            "56/56 - 4s - loss: 0.0371 - accuracy: 0.9888 - val_loss: 1.2765 - val_accuracy: 0.7551\n",
            "Epoch 12/30\n",
            "56/56 - 4s - loss: 0.0443 - accuracy: 0.9865 - val_loss: 1.0877 - val_accuracy: 0.7685\n",
            "Epoch 13/30\n",
            "56/56 - 4s - loss: 0.0104 - accuracy: 0.9972 - val_loss: 1.1695 - val_accuracy: 0.7753\n",
            "Epoch 14/30\n",
            "56/56 - 4s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.1613 - val_accuracy: 0.7820\n",
            "Epoch 15/30\n",
            "56/56 - 4s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2012 - val_accuracy: 0.7820\n",
            "Epoch 16/30\n",
            "56/56 - 4s - loss: 6.7670e-04 - accuracy: 1.0000 - val_loss: 1.2212 - val_accuracy: 0.7820\n",
            "Epoch 17/30\n",
            "56/56 - 4s - loss: 5.1512e-04 - accuracy: 1.0000 - val_loss: 1.2374 - val_accuracy: 0.7843\n",
            "Epoch 18/30\n",
            "56/56 - 4s - loss: 4.1132e-04 - accuracy: 1.0000 - val_loss: 1.2541 - val_accuracy: 0.7843\n",
            "Epoch 19/30\n",
            "56/56 - 4s - loss: 3.3947e-04 - accuracy: 1.0000 - val_loss: 1.2696 - val_accuracy: 0.7888\n",
            "Epoch 20/30\n",
            "56/56 - 4s - loss: 2.8577e-04 - accuracy: 1.0000 - val_loss: 1.2834 - val_accuracy: 0.7865\n",
            "Epoch 21/30\n",
            "56/56 - 4s - loss: 2.4454e-04 - accuracy: 1.0000 - val_loss: 1.2930 - val_accuracy: 0.7888\n",
            "Epoch 22/30\n",
            "56/56 - 4s - loss: 2.1226e-04 - accuracy: 1.0000 - val_loss: 1.3029 - val_accuracy: 0.7910\n",
            "Epoch 23/30\n",
            "56/56 - 4s - loss: 1.8653e-04 - accuracy: 1.0000 - val_loss: 1.3091 - val_accuracy: 0.7933\n",
            "Epoch 24/30\n",
            "56/56 - 4s - loss: 1.6487e-04 - accuracy: 1.0000 - val_loss: 1.3241 - val_accuracy: 0.7910\n",
            "Epoch 25/30\n",
            "56/56 - 4s - loss: 1.4784e-04 - accuracy: 1.0000 - val_loss: 1.3322 - val_accuracy: 0.7933\n",
            "Epoch 26/30\n",
            "56/56 - 4s - loss: 1.2921e-04 - accuracy: 1.0000 - val_loss: 1.3427 - val_accuracy: 0.7955\n",
            "Epoch 27/30\n",
            "56/56 - 4s - loss: 1.1452e-04 - accuracy: 1.0000 - val_loss: 1.3530 - val_accuracy: 0.7978\n",
            "Epoch 28/30\n",
            "56/56 - 4s - loss: 1.0156e-04 - accuracy: 1.0000 - val_loss: 1.3621 - val_accuracy: 0.7955\n",
            "Epoch 29/30\n",
            "56/56 - 4s - loss: 9.0378e-05 - accuracy: 1.0000 - val_loss: 1.3758 - val_accuracy: 0.7955\n",
            "Epoch 30/30\n",
            "56/56 - 4s - loss: 8.0565e-05 - accuracy: 1.0000 - val_loss: 1.3847 - val_accuracy: 0.7978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHAZ0-1djoTg"
      },
      "source": [
        "Syarat Kelulusan ✅\n",
        "- [x] Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel.\n",
        "- [x] Harus menggunakan LSTM dalam arsitektur model.\n",
        "- [x] Harus menggunakan model sequential.\n",
        "- [x] Validation set sebesar 20% dari total dataset.\n",
        "- [x] Harus menggunakan Embedding.\n",
        "- [x] Harus menggunakan fungsi tokenizer.\n",
        "- [x] Akurasi dari model minimal 75%.\n",
        "\n",
        "Syarat ⭐⭐⭐⭐⭐\n",
        "- [x] Lebih dari 3 Kelas\n",
        "- [x] Minimal 2000 Sampel Data\n",
        "- [x] Akurasi > 90%"
      ]
    }
  ]
}